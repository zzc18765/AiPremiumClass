{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T18:37:28.744210Z",
     "iopub.status.busy": "2025-05-30T18:37:28.743922Z",
     "iopub.status.idle": "2025-05-30T18:38:00.026532Z",
     "shell.execute_reply": "2025-05-30T18:38:00.025690Z",
     "shell.execute_reply.started": "2025-05-30T18:37:28.744184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer,DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate  \n",
    "from datasets import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:08:56.671769Z",
     "iopub.status.busy": "2025-05-30T19:08:56.671442Z",
     "iopub.status.idle": "2025-05-30T19:08:57.716393Z",
     "shell.execute_reply": "2025-05-30T19:08:57.715771Z",
     "shell.execute_reply.started": "2025-05-30T19:08:56.671737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n",
       "        num_rows: 45001\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'knowledge'],\n",
       "        num_rows: 3443\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载hf中dataset\n",
    "ds = load_dataset('ds_msra_ner')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:09:04.801166Z",
     "iopub.status.busy": "2025-05-30T19:09:04.800898Z",
     "iopub.status.idle": "2025-05-30T19:09:04.806643Z",
     "shell.execute_reply": "2025-05-30T19:09:04.806008Z",
     "shell.execute_reply.started": "2025-05-30T19:09:04.801148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['当', '希', '望', '工', '程', '救', '助', '的', '百', '万', '儿', '童', '成', '长', '起', '来', '，', '科', '教', '兴', '国', '蔚', '然', '成', '风', '时', '，', '今', '天', '有', '收', '藏', '价', '值', '的', '书', '你', '没', '买', '，', '明', '日', '就', '叫', '你', '悔', '不', '当', '初', '！']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "for items in ds['train']:\n",
    "    print(items['tokens'])\n",
    "    print(items['ner_tags'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T18:38:03.070126Z",
     "iopub.status.busy": "2025-05-30T18:38:03.069869Z",
     "iopub.status.idle": "2025-05-30T18:38:04.097288Z",
     "shell.execute_reply": "2025-05-30T18:38:04.096450Z",
     "shell.execute_reply.started": "2025-05-30T18:38:03.070106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实体映射字典\n",
    "\n",
    "'O':0   \n",
    "'B-PER':1   \n",
    "'I-PER':2   \n",
    "'B-LOC':3   \n",
    "'I-LOC':4   \n",
    "'B-ORG':5   \n",
    "'I-ORG':6   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T18:38:04.098856Z",
     "iopub.status.busy": "2025-05-30T18:38:04.098185Z",
     "iopub.status.idle": "2025-05-30T18:38:09.206365Z",
     "shell.execute_reply": "2025-05-30T18:38:09.205544Z",
     "shell.execute_reply.started": "2025-05-30T18:38:04.098824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证tag标签数量\n",
    "tags_id = set()\n",
    "for items in ds['train']:\n",
    "    tags_id.update(items['ner_tags'])\n",
    "    \n",
    "tags_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T18:38:09.207350Z",
     "iopub.status.busy": "2025-05-30T18:38:09.207115Z",
     "iopub.status.idle": "2025-05-30T18:38:09.212553Z",
     "shell.execute_reply": "2025-05-30T18:38:09.211977Z",
     "shell.execute_reply.started": "2025-05-30T18:38:09.207330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# entity_index\n",
    "entites = ['O'] + list({'PER', 'LOC', 'ORG'})\n",
    "tags = ['O']\n",
    "for entity in entites[1:]:\n",
    "    tags.append('B-' + entity.upper())\n",
    "    tags.append('I-' + entity.upper())\n",
    "\n",
    "entity_index = {entity:i for i, entity in enumerate(entites)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T18:38:09.214157Z",
     "iopub.status.busy": "2025-05-30T18:38:09.213635Z",
     "iopub.status.idle": "2025-05-30T18:38:09.241207Z",
     "shell.execute_reply": "2025-05-30T18:38:09.240641Z",
     "shell.execute_reply.started": "2025-05-30T18:38:09.214138Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'ORG': 1, 'PER': 2, 'LOC': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T18:38:09.242098Z",
     "iopub.status.busy": "2025-05-30T18:38:09.241838Z",
     "iopub.status.idle": "2025-05-30T18:38:09.255885Z",
     "shell.execute_reply": "2025-05-30T18:38:09.255305Z",
     "shell.execute_reply.started": "2025-05-30T18:38:09.242081Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:44:18.424266Z",
     "iopub.status.busy": "2025-05-30T19:44:18.422816Z",
     "iopub.status.idle": "2025-05-30T19:44:30.778739Z",
     "shell.execute_reply": "2025-05-30T19:44:30.777951Z",
     "shell.execute_reply.started": "2025-05-30T19:44:18.424240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def data_input_proc(item):\n",
    "    # 文本已经分为字符，且tag索引也已经提供\n",
    "    # 所以数据预处理反而简单\n",
    "    # 导入已拆分为字符的文本列表，需要设置参数is_split_into_words=True\n",
    "    input_data = tokenizer(item['tokens'], \n",
    "                           truncation=True,\n",
    "                           add_special_tokens=False, \n",
    "                           max_length=512, \n",
    "                           is_split_into_words=True)\n",
    "    \n",
    "    labels = [lbl[:512] for lbl in item['ner_tags']]\n",
    "    input_data['labels'] = labels\n",
    "    return input_data\n",
    "\n",
    "ds1 = ds.map(data_input_proc, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:46:30.212252Z",
     "iopub.status.busy": "2025-05-30T19:46:30.211988Z",
     "iopub.status.idle": "2025-05-30T19:46:30.217746Z",
     "shell.execute_reply": "2025-05-30T19:46:30.217114Z",
     "shell.execute_reply.started": "2025-05-30T19:46:30.212232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds1.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:46:32.078923Z",
     "iopub.status.busy": "2025-05-30T19:46:32.078623Z",
     "iopub.status.idle": "2025-05-30T19:46:32.086430Z",
     "shell.execute_reply": "2025-05-30T19:46:32.085755Z",
     "shell.execute_reply.started": "2025-05-30T19:46:32.078900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([2496, 2361, 3307, 2339, 4923, 3131, 1221, 4638, 4636,  674, 1036, 4997,\n",
      "        2768, 7270, 6629, 3341, 8024, 4906, 3136, 1069, 1744, 5917, 4197, 2768,\n",
      "        7599, 3198, 8024,  791, 1921, 3300, 3119, 5966,  817,  966, 4638,  741,\n",
      "         872, 3766,  743, 8024, 3209, 3189, 2218, 1373,  872, 2637,  679, 2496,\n",
      "        1159, 8013]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1]), 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])}\n"
     ]
    }
   ],
   "source": [
    "for item in ds1['train']:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:38:49.136866Z",
     "iopub.status.busy": "2025-05-30T19:38:49.136299Z",
     "iopub.status.idle": "2025-05-30T19:38:49.418337Z",
     "shell.execute_reply": "2025-05-30T19:38:49.417669Z",
     "shell.execute_reply.started": "2025-05-30T19:38:49.136842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2lbl = {i:tag for i, tag in enumerate(tags)}\n",
    "lbl2id = {tag:i for i, tag in enumerate(tags)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('bert-base-chinese', \n",
    "                                                        num_labels=len(tags),\n",
    "                                                        id2label=id2lbl,\n",
    "                                                        label2id=lbl2id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练 TrainningArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:48:30.130219Z",
     "iopub.status.busy": "2025-05-30T19:48:30.129941Z",
     "iopub.status.idle": "2025-05-30T19:48:30.164373Z",
     "shell.execute_reply": "2025-05-30T19:48:30.163584Z",
     "shell.execute_reply.started": "2025-05-30T19:48:30.130198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"msra_ner_train\",  # 模型训练工作目录（tensorboard，临时模型存盘文件，日志）\n",
    "    num_train_epochs = 3,    # 训练 epoch\n",
    "    # save_safetensors=False,  # 设置False保存文件可以通过torch.load加载\n",
    "    per_device_train_batch_size=32,  # 训练批次\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to='tensorboard',  # 训练输出记录\n",
    "    eval_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练 Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:38:55.250510Z",
     "iopub.status.busy": "2025-05-30T19:38:55.250240Z",
     "iopub.status.idle": "2025-05-30T19:38:55.256116Z",
     "shell.execute_reply": "2025-05-30T19:38:55.255531Z",
     "shell.execute_reply.started": "2025-05-30T19:38:55.250488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# metric 方法\n",
    "def compute_metric(result):\n",
    "    # result 是一个tuple (predicts, labels)\n",
    "    \n",
    "    # 获取评估对象\n",
    "    seqeval = evaluate.load('seqeval')\n",
    "    predicts,labels = result\n",
    "    predicts = np.argmax(predicts, axis=2)\n",
    "    \n",
    "    # 准备评估数据\n",
    "    predicts = [[tags[p] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    labels = [[tags[l] for p,l in zip(ps,ls) if l != -100]\n",
    "                 for ps,ls in zip(predicts,labels)]\n",
    "    results = seqeval.compute(predictions=predicts, references=labels)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate \n",
    "# evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:38:57.290214Z",
     "iopub.status.busy": "2025-05-30T19:38:57.289638Z",
     "iopub.status.idle": "2025-05-30T19:38:57.293711Z",
     "shell.execute_reply": "2025-05-30T19:38:57.292896Z",
     "shell.execute_reply.started": "2025-05-30T19:38:57.290188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:48:33.490332Z",
     "iopub.status.busy": "2025-05-30T19:48:33.490032Z",
     "iopub.status.idle": "2025-05-30T19:48:33.503665Z",
     "shell.execute_reply": "2025-05-30T19:48:33.503038Z",
     "shell.execute_reply.started": "2025-05-30T19:48:33.490310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=ds1['train'],\n",
    "    eval_dataset=ds1['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T19:48:35.203823Z",
     "iopub.status.busy": "2025-05-30T19:48:35.203047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2112' max='2112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2112/2112 18:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loc</th>\n",
       "      <th>Org</th>\n",
       "      <th>Per</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>{'precision': 0.9412798874824191, 'recall': 0.9386395511921458, 'f1': 0.9399578651685394, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.939894319682959, 'recall': 0.9467731204258151, 'f1': 0.9433211799801128, 'number': 1503}</td>\n",
       "      <td>{'precision': 0.8098542678695351, 'recall': 0.884090909090909, 'f1': 0.8453458891705904, 'number': 1320}</td>\n",
       "      <td>0.908260</td>\n",
       "      <td>0.928106</td>\n",
       "      <td>0.918076</td>\n",
       "      <td>0.991366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.029246</td>\n",
       "      <td>{'precision': 0.9527195165303946, 'recall': 0.9396914446002805, 'f1': 0.9461606354810238, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.9425587467362925, 'recall': 0.9607451763140386, 'f1': 0.9515650741350906, 'number': 1503}</td>\n",
       "      <td>{'precision': 0.856638418079096, 'recall': 0.918939393939394, 'f1': 0.8866959064327485, 'number': 1320}</td>\n",
       "      <td>0.926402</td>\n",
       "      <td>0.940441</td>\n",
       "      <td>0.933368</td>\n",
       "      <td>0.992588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.032138</td>\n",
       "      <td>{'precision': 0.9519604380077711, 'recall': 0.9449509116409537, 'f1': 0.9484427239134259, 'number': 2852}</td>\n",
       "      <td>{'precision': 0.9466089466089466, 'recall': 0.8729208250166334, 'f1': 0.9082727587400484, 'number': 1503}</td>\n",
       "      <td>{'precision': 0.8678038379530917, 'recall': 0.925, 'f1': 0.8954895489548955, 'number': 1320}</td>\n",
       "      <td>0.929587</td>\n",
       "      <td>0.921233</td>\n",
       "      <td>0.925392</td>\n",
       "      <td>0.992335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 6.34k/6.34k [00:00<00:00, 16.3MB/s]\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9412798874824191, 'recall': 0.9386395511921458, 'f1': 0.9399578651685394, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.939894319682959, 'recall': 0.9467731204258151, 'f1': 0.9433211799801128, 'number': 1503}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8098542678695351, 'recall': 0.884090909090909, 'f1': 0.8453458891705904, 'number': 1320}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9527195165303946, 'recall': 0.9396914446002805, 'f1': 0.9461606354810238, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9425587467362925, 'recall': 0.9607451763140386, 'f1': 0.9515650741350906, 'number': 1503}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.856638418079096, 'recall': 0.918939393939394, 'f1': 0.8866959064327485, 'number': 1320}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9519604380077711, 'recall': 0.9449509116409537, 'f1': 0.9484427239134259, 'number': 2852}\" of type <class 'dict'> for key \"eval/LOC\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.9466089466089466, 'recall': 0.8729208250166334, 'f1': 0.9082727587400484, 'number': 1503}\" of type <class 'dict'> for key \"eval/ORG\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'precision': 0.8678038379530917, 'recall': 0.925, 'f1': 0.8954895489548955, 'number': 1320}\" of type <class 'dict'> for key \"eval/PER\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2112, training_loss=0.02358800316737457, metrics={'train_runtime': 1111.0185, 'train_samples_per_second': 121.513, 'train_steps_per_second': 1.901, 'total_flos': 1.180990200098808e+16, 'train_loss': 0.02358800316737457, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型推理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline('token-classification', 'msra_ner_train/checkpoint-2112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-LOC',\n",
       "  'score': np.float32(0.99652195),\n",
       "  'index': 10,\n",
       "  'word': '中',\n",
       "  'start': 9,\n",
       "  'end': 10},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.99696547),\n",
       "  'index': 11,\n",
       "  'word': '美',\n",
       "  'start': 10,\n",
       "  'end': 11}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline('双方确定了今后发展中美关系的指导方针')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
