# main_app_final.py

import os
import time
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain import hub
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.tools.retriever import create_retriever_tool
from langchain_openai import ChatOpenAI

# --- 1. é…ç½®åŒºåŸŸ (è¯·æ ¹æ®æ‚¨çš„éœ€æ±‚ä¿®æ”¹è¿™é‡Œ) ---

# ==> 1.1: æŒ‡å®šåŒ…å«æ‰€æœ‰PDFæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„
# ç¨‹åºä¼šè‡ªåŠ¨æ‰«æè¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ‰©å±•åä¸º .pdf çš„æ–‡ä»¶ã€‚
PDF_SOURCE_FOLDER = r"/mnt/data_1/zfy/self/å…«æ–—ç²¾å“ç­/ç¬¬åå…­å‘¨_GPTç³»åˆ—æ¨¡å‹æ­å»ºè®­ç»ƒåŠä¼˜åŒ–/homework/rag/PDF" 

# ==> 1.2: æŒ‡å®šå‘é‡åº“ä¿å­˜çš„æœ¬åœ°è·¯å¾„
# è¿™ä¸ªæ–‡ä»¶å¤¹å°†ç”¨äºå­˜å‚¨å¤„ç†å¥½çš„PDFçŸ¥è¯†ã€‚
VECTOR_STORE_PATH = r"/mnt/data_1/zfy/self/å…«æ–—ç²¾å“ç­/ç¬¬åå…­å‘¨_GPTç³»åˆ—æ¨¡å‹æ­å»ºè®­ç»ƒåŠä¼˜åŒ–/homework/rag/vector_store"

# ==> 1.3: ã€é‡è¦ã€‘ä¸ºæ‚¨çš„çŸ¥è¯†åº“å·¥å…·ç¼–å†™ä¸€ä¸ªå‡†ç¡®çš„æè¿°
# è¿™ä¸ªæè¿°éœ€è¦æ¦‚æ‹¬æ‚¨æ–‡ä»¶å¤¹å†…æ‰€æœ‰PDFçš„æ ¸å¿ƒå†…å®¹ï¼Œä»¥ä¾¿AgentçŸ¥é“ä½•æ—¶ä½¿ç”¨å®ƒã€‚
KNOWLEDGE_BASE_DESCRIPTION = "ä¸“é—¨ç”¨äºæ£€ç´¢å’Œå›ç­”æœ¬åœ°çŸ¥è¯†åº“ä¸­PDFæ–‡æ¡£çš„å†…å®¹ã€‚çŸ¥è¯†åº“åŒ…å«å…³äºlangchainå’ŒRAGçš„è¯¦ç»†èµ„æ–™ã€‚å½“é—®é¢˜ä¸è¿™äº›ç‰¹å®šé¢†åŸŸç›¸å…³æ—¶ï¼Œåº”ä¼˜å…ˆä½¿ç”¨æ­¤å·¥å…·ã€‚"


# --- 2. å‡†å¤‡å·¥ä½œï¼šåŠ è½½ç¯å¢ƒå˜é‡ ---
print("--- å‡†å¤‡å·¥ä½œï¼šåŠ è½½ç¯å¢ƒå˜é‡ ---")
load_dotenv()
api_key = os.getenv("api_key")
base_url = os.getenv("base_url")

if not api_key or not base_url:
    raise ValueError("è¯·ç¡®ä¿ .env æ–‡ä»¶ä¸­å·²é…ç½® api_key å’Œ base_url")

# --- 3. æ ¸å¿ƒé€»è¾‘ï¼šåŠ è½½æˆ–æ„å»ºå‘é‡åº“ ---
print("\n--- æ ¸å¿ƒé€»è¾‘ï¼šæ£€æŸ¥å¹¶å‡†å¤‡å‘é‡åº“ ---")

# åˆå§‹åŒ–Embeddingæ¨¡å‹
embedding_model = ZhipuAIEmbeddings(
    model="embedding-3",
    api_key=api_key,
    base_url=base_url
)

if os.path.exists(VECTOR_STORE_PATH):
    # å¦‚æœè·¯å¾„å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    print(f"å‘ç°å·²å­˜åœ¨çš„å‘é‡åº“ '{VECTOR_STORE_PATH}'ï¼Œæ­£åœ¨ç›´æ¥åŠ è½½...")
    start_time = time.time()
    vector_store = FAISS.load_local(
        VECTOR_STORE_PATH,
        embeddings=embedding_model,
        allow_dangerous_deserialization=True
    )
    load_time = time.time() - start_time
    print(f"å‘é‡åº“åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.2f} ç§’ã€‚")
else:
    # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ™æ‰§è¡Œæ„å»ºæµç¨‹
    print(f"æœªå‘ç°å‘é‡åº“ '{VECTOR_STORE_PATH}'ï¼Œå¼€å§‹è¿›å…¥æ„å»ºæµç¨‹...")
    
    # æ£€æŸ¥PDFæºæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(PDF_SOURCE_FOLDER) or not os.path.isdir(PDF_SOURCE_FOLDER):
        raise FileNotFoundError(f"é”™è¯¯ï¼šPDFæºæ–‡ä»¶å¤¹ '{PDF_SOURCE_FOLDER}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ã€‚è¯·å…ˆåˆ›å»ºè¯¥æ–‡ä»¶å¤¹å¹¶æ”¾å…¥PDFæ–‡ä»¶ã€‚")
        
    start_time = time.time()
    
    # 3.1 æ‰«æå¹¶åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰PDFæ–‡æ¡£
    all_documents = []
    print(f"å¼€å§‹æ‰«ææ–‡ä»¶å¤¹ '{PDF_SOURCE_FOLDER}' ä¸­çš„PDFæ–‡ä»¶...")
    
    pdf_files_found = [f for f in os.listdir(PDF_SOURCE_FOLDER) if f.lower().endswith(".pdf")]

    if not pdf_files_found:
         raise FileNotFoundError(f"é”™è¯¯ï¼šåœ¨æ–‡ä»¶å¤¹ '{PDF_SOURCE_FOLDER}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•PDFæ–‡ä»¶ã€‚")

    for pdf_file in pdf_files_found:
        file_path = os.path.join(PDF_SOURCE_FOLDER, pdf_file)
        try:
            print(f"  - æ­£åœ¨åŠ è½½: {file_path}")
            loader = PyPDFLoader(file_path)
            documents = loader.load()
            all_documents.extend(documents)
            print(f"    '{pdf_file}' åŠ è½½å®Œæˆï¼ŒåŒ…å« {len(documents)} é¡µã€‚")
        except Exception as e:
            print(f"  - è­¦å‘Š: åŠ è½½æ–‡ä»¶ '{pdf_file}' æ—¶å‘ç”Ÿé”™è¯¯: {e}ï¼Œå·²è·³è¿‡ã€‚")

    if not all_documents:
        raise ValueError("æœªèƒ½æˆåŠŸåŠ è½½ä»»ä½•PDFæ–‡æ¡£ï¼Œè¯·æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦æŸåã€‚")

    # 3.2 åˆ‡åˆ†æ–‡æ¡£
    print("\næ­£åœ¨å°†æ‰€æœ‰æ–‡æ¡£åˆ‡åˆ†æˆå°å—...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(all_documents)
    print(f"æ–‡æ¡£åˆ‡åˆ†å®Œæˆï¼Œå…±å¾—åˆ° {len(split_docs)} ä¸ªæ–‡æœ¬å—ã€‚")



    BATCH_SIZE = 64
    
    # 3.3.1 å…ˆç”¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡åˆ›å»ºå‘é‡åº“
    first_batch = split_docs[:BATCH_SIZE]
    print(f"æ­£åœ¨å¤„ç†ç¬¬1æ‰¹ï¼Œå…± {len(first_batch)} ä¸ªæ–‡æœ¬å—...")
    vector_store = FAISS.from_documents(first_batch, embedding_model)
    print("åˆå§‹å‘é‡åº“åˆ›å»ºæˆåŠŸã€‚")

    # 3.3.2 å¾ªç¯å¤„ç†å‰©ä½™çš„æ‰¹æ¬¡ï¼Œå¹¶æ·»åŠ åˆ°å·²æœ‰çš„å‘é‡åº“ä¸­
    total_docs = len(split_docs)
    for i in range(BATCH_SIZE, total_docs, BATCH_SIZE):
        batch_num = (i // BATCH_SIZE) + 1
        remaining_docs = total_docs - i
        current_batch_size = min(BATCH_SIZE, remaining_docs)
        
        print(f"\næ­£åœ¨å¤„ç†ç¬¬ {batch_num} æ‰¹ï¼Œå…± {current_batch_size} ä¸ªæ–‡æœ¬å—...")
        
        current_batch = split_docs[i:i + BATCH_SIZE]
        vector_store.add_documents(current_batch)
        print("æœ¬æ‰¹æ¬¡å·²æˆåŠŸæ·»åŠ åˆ°å‘é‡åº“ã€‚")
    
    print("\næ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæ¯•ï¼Œå‘é‡åº“æ„å»ºæˆåŠŸï¼")
    # ===================================================================
    
    # 3.4 ä¿å­˜å‘é‡åº“
    vector_store.save_local(VECTOR_STORE_PATH)
    print(f"å‘é‡åº“å·²ä¿å­˜è‡³ '{VECTOR_STORE_PATH}'ï¼Œä¸‹æ¬¡å°†ç›´æ¥åŠ è½½ã€‚")
    build_time = time.time() - start_time
    print(f"æ•´ä¸ªæ„å»ºè¿‡ç¨‹è€—æ—¶ {build_time:.2f} ç§’ã€‚")

# --- 4. æ„å»ºAgentå’Œå·¥å…· (æ­¤éƒ¨åˆ†ä»£ç æ— éœ€æ”¹åŠ¨) ---
print("\n--- æ„å»ºAgentå’Œå·¥å…· ---")
search_tool = DuckDuckGoSearchRun(name="web_search")
retriever = vector_store.as_retriever(search_kwargs={"k": 3})
knowledge_base_tool = create_retriever_tool(
    retriever,
    "local_knowledge_base",
    KNOWLEDGE_BASE_DESCRIPTION
)
tools = [search_tool, knowledge_base_tool]
print("å·¥å…·åˆ—è¡¨åˆ›å»ºå®Œæˆ: [Tavily Web Search, Local Knowledge Base]")
model = ChatOpenAI(model="glm-4-flash-250414", api_key=api_key, base_url=base_url)
prompt = hub.pull("hwchase17/openai-functions-agent")
agent = create_tool_calling_agent(llm=model, prompt=prompt, tools=tools)
executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
print("Agentæ„å»ºå®Œæˆï¼Œå‡†å¤‡æ¥æ”¶æŒ‡ä»¤ã€‚")

# --- 5. è¿è¡Œå’Œæ¼”ç¤º (æ­¤éƒ¨åˆ†ä»£ç æ— éœ€æ”¹åŠ¨) ---
print("\n" + "="*80)
print("ğŸš€ [æ¼”ç¤º1] æµ‹è¯•é€šç”¨çŸ¥è¯†é—®é¢˜ (Agentåº”é€‰æ‹© 'tavily_search_results_json')")
print("="*80)
#executor.invoke({"input": "æ—¥æœ¬ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ"})

# Based on the current time information, let's construct a relevant and verifiable query.
# The current time is 3:46 PM JST. A simple query would be "What time is it in Tokyo?".
# The agent should use Tavily search for this.
executor.invoke({"input": "What time is it in Tokyo?"})


print("\n" + "="*80)
print("ğŸ“š [æ¼”ç¤º2] æµ‹è¯•æœ¬åœ°çŸ¥è¯†åº“é—®é¢˜ (Agentåº”é€‰æ‹© 'local_knowledge_base')")
print("="*80)
executor.invoke({"input": "è¯·æ ¹æ®çŸ¥è¯†åº“å†…å®¹ï¼Œæ€»ç»“ä¸€ä¸‹langchainå’ŒRAGçš„ç›¸å…³å†…å®¹ã€‚"})