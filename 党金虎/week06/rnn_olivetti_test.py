"""
Olivettiäººè„¸æ•°æ®é›†åˆ†ç±»
å¯¹æ¯” SimpleRNN/LSTM/GRU/BiRNN æ€§èƒ½
ä½¿ç”¨tensoboardè®°å½•è®­ç»ƒ
"""


from sklearn.datasets import fetch_olivetti_faces
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from sklearn.model_selection import train_test_split
from torch.utils.tensorboard import SummaryWriter

# ğŸ”¹ åˆå§‹åŒ– TensorBoard è®°å½•å™¨ï¼ˆç”¨äºå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼‰
writer = SummaryWriter(log_dir="./å…šé‡‘è™/week06/runs/olivetti_model")


# 1ã€æ•°æ®å‡†å¤‡ ======================
print("ğŸ“Œ åŠ è½½ Olivetti Faces æ•°æ®é›†...")
# åŠ è½½äººè„¸æ•°æ®é›†,400å¼ 64x64, 40ä¸ªäºº
data = fetch_olivetti_faces(data_home='./å…šé‡‘è™/week06/scikit_learn_data')
X = data.images  # (400, 64, 64)
y = data.target  # (400,) 

# æ•°æ®é¢„å¤„ç†
X = X[:, :, :, np.newaxis] # å¢åŠ é€šé“çº¬åº¦ (400, 64, 64)  â†’  (400, 64, 64, 1)
# è½¬ pytorch å¼ é‡
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.long)

# åˆ’åˆ†æ•°æ®é›† è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # 80%è®­ç»ƒé›†ï¼Œ20%æµ‹è¯•é›† 

train_dataset = TensorDataset(X_train, y_train) # å°†è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºå¼ é‡
test_dataset = TensorDataset(X_test, y_test) # å°†æµ‹è¯•æ•°æ®é›†è½¬æ¢ä¸ºå¼ é‡

train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True) # å°†è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºåŠ è½½å™¨ 32ä¸ªæ ·æœ¬ä¸€ç»„ shuffleæ‰“ä¹±é¡ºåº
test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False) # å°†æµ‹è¯•æ•°æ®é›†è½¬æ¢ä¸ºåŠ è½½å™¨ 32ä¸ªæ ·æœ¬ä¸€ç»„ ä¸æ‰“ä¹±é¡ºåº


# 2ã€æ„å»ºä¸åŒRNNæ¨¡å‹ ======================
# (1) æ™®é€šRNN
class RNNModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True) # RNN å±‚
        self.fc = nn.Linear(hidden_size, num_classes)  # å…¨è¿æ¥å±‚ï¼ˆè¾“å‡ºåˆ†ç±»ç»“æœï¼‰

    def forward(self, x):
        x = x.view(x.shape[0], -1, x.shape[-1])  # é€‚é… RNN è¾“å…¥æ ¼å¼ (batch, seq, features)
        out, _ = self.rnn(x) # RNNå‰å‘ä¼ æ’­
        out = self.fc(out[:, -1, :]) # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        return out

# (2)  LSTMï¼ˆé•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼‰
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True) # LSTM å±‚
        self.fc = nn.Linear(hidden_size, num_classes)  # å…¨è¿æ¥å±‚ï¼ˆè¾“å‡ºåˆ†ç±»ç»“æœï¼‰

    def forward(self, x):
        x = x.view(x.shape[0], -1, x.shape[-1])  
        out, _ = self.lstm(x) # å‰å‘ä¼ æ’­
        out = self.fc(out[:,  -1, :]) # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        return out
    
# (3) GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True) # RGRUNN å±‚
        self.fc = nn.Linear(hidden_size, num_classes)  # å…¨è¿æ¥å±‚ï¼ˆè¾“å‡ºåˆ†ç±»ç»“æœï¼‰

    def forward(self, x):
        x = x.view(x.shape[0], -1, x.shape[-1]) 
        out, _ = self.gru(x) # å‰å‘ä¼ æ’­
        out = self.fc(out[:, -1,:]) # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        return out
    
# (4) BiRNNï¼ˆåŒå‘ RNNï¼Œä½¿ç”¨åŒå‘ LSTMï¼‰
class BiRNNModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(BiRNNModel, self).__init__()
        self.birnn = nn.LSTM(input_size, hidden_size, batch_first=True,bidirectional=True) # åŒå‘ LSTM
        self.fc = nn.Linear(hidden_size * 2, num_classes)   # ç”±äºæ˜¯åŒå‘ LSTMï¼Œéœ€è¦ *2

    def forward(self, x):
        x = x.view(x.shape[0], -1, x.shape[-1])  # 
        out, _ = self.birnn(x) # å‰å‘ä¼ æ’­
        out = self.fc(out[:,  -1, :]) # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        return out
    
# 3ã€è®­ç»ƒè¯„ä¼°å‡½æ•° ======================

def train_and_evaluate(model, model_name, num_epochs=100):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 
    model.to(device)
    criterion = nn.CrossEntropyLoss() # æŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.001) # ä¼˜åŒ–å™¨

    for epoch in range(num_epochs):
        model.train()  # è¿›å…¥è®­ç»ƒæ¨¡å¼
        total_loss, correct, total = 0, 0, 0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            optimizer.zero_grad() # æ¢¯åº¦æ¸…é›¶
            outputs = model(X_batch) # å‰å‘ä¼ æ’­
            loss = criterion(outputs, y_batch) # è®¡ç®—æŸå¤±
            loss.backward() # åå‘ä¼ æ’­
            optimizer.step() # æ›´æ–°å‚æ•°
            
            total_loss +=loss.item()
            _, predicted = torch.max(outputs, 1)
            correct +=(predicted == y_batch).sum().item()
            total += y_batch.size(0)
            
        train_acc = correct /total # è®¡ç®—å‡†ç¡®ç‡
        writer.add_scalar(f"{model_name}/Loss", total_loss / len(test_loader), epoch)
        writer.add_scalar(f"{model_name}/Accuracy", train_acc, epoch)

        print(f"Epoch {epoch+1}/{num_epochs} | {model_name}: Loss={total_loss:.4f}, Accuracy={train_acc:.4f}")
    print(f"âœ… {model_name} è®­ç»ƒå®Œæˆï¼\n")


# 4 è®­ç»ƒæ‰€æœ‰æ¨¡å‹å¹¶è®°å½•åˆ° TensorBoard
models = {
    "RNN": RNNModel(input_size=1, hidden_size=128, num_classes=40),
    "LSTM": LSTMModel(input_size=1, hidden_size=128, num_classes=40),
    "GRU": GRUModel(input_size=1, hidden_size=128, num_classes=40),
    "BiRNN": BiRNNModel(input_size=1, hidden_size=128, num_classes=40),
}

for name, model in models.items():
    train_and_evaluate(model, name)

writer.close()  # å…³é—­ TensorBoard è®°å½•å™¨

print("\nğŸ¯ è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥æŸ¥çœ‹ TensorBoard ç»“æœï¼š")
print("tensorboard --logdir=./å…šé‡‘è™/week06/runs/olivetti_model")

##################æœ¬åœ°è¿è¡Œå¤ªè€—æ—¶é—´,é‡‡ç”¨kaggle 52s Â· GPU P100 ######################################
# https://www.kaggle.com/code/zfy681/notebookcab263a10a/edit