'''
ä½¿ç”¨RNNå®ç°ä¸€ä¸ªå¤©æ°”é¢„æµ‹æ¨¡å‹ï¼Œèƒ½é¢„æµ‹1å¤©å’Œè¿ç»­5å¤©çš„æœ€é«˜æ°”æ¸©
'''


import numpy as np
import torch
import torch.nn as nn
import pandas as pd
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader,TensorDataset
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# 1. åŠ è½½æ•°æ®==========================================
def load_and_preprocess_data(csv_path,look_back=7, future_steps=1):
    """
    è¯»å–å¤©æ°”æ•°æ®ï¼Œé¢„å¤„ç†å¹¶è¿”å› DataLoader
    :param csv_path: CSV æ–‡ä»¶è·¯å¾„
    :param look_back: ç”¨è¿‡å»Nå¤©é¢„æµ‹æœªæ¥
    :param future_steps: é¢„æµ‹æœªæ¥Nå¤©
    :return: è®­ç»ƒé›† & æµ‹è¯•é›† DataLoader
    """
    data = pd.read_csv(csv_path,low_memory=False)
 
    #  å–æœ€é«˜æ¸© æ•°æ®é¢„æ¸…æ´—,å»é™¤æ— æ•ˆå€¼NaN
    data = data[['Date', 'MaxTemp']].dropna()
    # å½’ä¸€åŒ–æœ€é«˜æ°”æ¸©
    scaler = MinMaxScaler(feature_range=(0,1))
    data['MaxTemp'] = scaler.fit_transform(data['MaxTemp'].values.reshape(-1,1))
    # æ—¥æœŸè½¬æ¢
    data['Date'] = pd.to_datetime(data['Date'])
    data.set_index('Date', inplace=True)


    # æ•°æ®é¢„å¤„ç†, ä½¿ç”¨è¿‡å»Nå¤©çš„æ•°æ®é¢„æµ‹æœªæ¥çš„æ°”æ¸©
    def create_dataset(data, look_back, fulture_steps): # look_back å‚æ•°å†³å®šäº†æˆ‘ä»¬ä½¿ç”¨å¤šå°‘ä¸ªå†å²æ—¶é—´æ­¥çš„æ•°æ®æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ•°æ®
        X, y = [], []
        for i in range(len(data) - look_back - fulture_steps):
            X.append(data[i:i + look_back,0])
            y.append(data[i + look_back: i + look_back + fulture_steps,0])
        return np.array(X), np.array(y)

    # è®¾ç½®æ—¶é—´æ­¥, ä½¿ç”¨è¿‡å»7å¤©çš„æ°”æ¸©æ•°æ®é¢„æµ‹æœªæ¥æ°”æ¸©
    look_back = 7
    dataset = data.values
    X, y = create_dataset(dataset, look_back, future_steps)

    # æ•°æ®æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # è½¬æ¢æˆå¼ é‡ tensor
    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)
    y_train = torch.tensor(y_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)
    y_test = torch.tensor(y_test, dtype=torch.float32)

    # åˆ›å»ºDataLoader
    train_data = TensorDataset(X_train, y_train)
    test_data = TensorDataset(X_test, y_test)

    train_loader = DataLoader(train_data,batch_size=32,shuffle=True)
    test_loader = DataLoader(test_data, batch_size=32,shuffle=False)
    return train_loader, test_loader, scaler



# 2. æ„å»ºLSTMè®­ç»ƒæ¨¡å‹==========================================
class WeatherRNN(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, output_size=1):
        super(WeatherRNN, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, _ = self.lstm(x)  # out.shape: [batch, seq_len, hidden_size]
        return self.fc(out[:, -1, :])  # åªå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥



# 3. è®­ç»ƒæ¨¡å‹ & è¯„ä¼°è®­ç»ƒ, æŸå¤±å‡½æ•°è®¾ä¸ºMSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰  Adam ä¼˜åŒ–å™¨
def train_and_evaluate(model, train_loader, test_loader,epochs=100,lr=0.001,model_name="LSTM"):

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    writer = SummaryWriter(log_dir=f"./å…šé‡‘è™/week06/runs/weather_model/{model_name}")


    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            optimizer.zero_grad() # æ¢¯åº¦æ¸…é›¶
            outputs = model(X_batch) # å‰å‘ä¼ æ’­
            loss = criterion(outputs, y_batch) # è®¡ç®—æŸå¤±
            loss.backward() # åå‘ä¼ æ’­
            optimizer.step()  # æ›´æ–°å‚æ•°
            total_loss +=loss.item()
            
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_train_loss = total_loss / len(train_loader)
        writer.add_scalar(f"{model_name}/Train_Loss", avg_train_loss, epoch)
  
        # è¯„ä¼°
        model.eval()
        total_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in test_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs,y_batch)
                total_loss += loss.item()
        avg_test_loss = total_loss / len(test_loader)
        writer.add_scalar(f"{model_name}/Test_Loss", avg_test_loss, epoch)
        
        print(f"Epoch [{epoch+1}/{epochs}], Train_Loss: {avg_train_loss:.4f}, Test_Loss: {avg_test_loss:.4f} ")
    writer.close()
    

# 4.æ‰§è¡Œ==========================================
def run_experiment(csv_path, future_steps,epochs=100):
    """
    è¿è¡Œå¤©æ°”é¢„æµ‹å®éªŒ
    :param csv_path: æ•°æ®é›†è·¯å¾„
    :param future_steps: é¢„æµ‹æœªæ¥Nå¤©
    :param epochs: è®­ç»ƒè½®æ•°
    """
    print(f"ğŸ“Œ æ­£åœ¨è®­ç»ƒ {future_steps} å¤©é¢„æµ‹æ¨¡å‹...")
    train_loader, test_loader, _ = load_and_preprocess_data(csv_path,future_steps=future_steps)
    model = WeatherRNN(output_size = future_steps)
    train_and_evaluate(model,train_loader,test_loader,epochs,model_name=f"LSTM_{future_steps}_Days")


# 5.æµ‹è¯•==========================================
csv_path ="./å…šé‡‘è™/week06/weather_tem_data/Summary of Weather.csv"
run_experiment(csv_path,future_steps=1,epochs=10) # 1å¤©é¢„æµ‹
run_experiment(csv_path,future_steps=5,epochs=10) # 5å¤©é¢„æµ‹

print("\nğŸ¯ è¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥æŸ¥çœ‹ TensorBoard ç»“æœï¼š")
print("tensorboard --logdir=./å…šé‡‘è™/week06/runs/weather_model")