# æ¥ comments_process.py çš„ç”µå½±è¯„è®ºæ•°æ®å¤„ç† ä¿å­˜çš„ds_commerntsæ•°æ® å¯¹åº”çš„comments.pkl
# ç»§ç»­ä¸‹ä¸€æ­¥çš„ç”µå½±è¯„è®ºåˆ†ç±»ä»»åŠ¡  äºŒåˆ†ç±»ï¼ˆå¥½/åï¼‰

import torch
import torch.nn as nn
import pickle
import jieba
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence # é•¿åº¦ä¸åŒçš„å¼ é‡å¡«å……ä¸ºç›¸åŒé•¿åº¦ length pad_sequenceè¦æ±‚ä¼ å…¥æ•°æ®æ˜¯å¼ é‡

# 1 åŠ è½½è®­ç»ƒæ•°æ® ï¼ˆè¯­æ–™å’Œæ ‡ç­¾ï¼‰
with open('./comments.pkl','rb') as f:
    comment_data = pickle.load(f)



# 2 æ„å»ºè¯æ±‡è¡¨ è¯„è®ºè¯æ±‡çš„è¯æ±‡è¡¨æ„å»ºå’ŒæœªçŸ¥è¯æ±‡çš„è¡¥å…¨ å’Œ è¯„è®ºè¯æ±‡é•¿åº¦ï¼ˆå¤§å‹è¯å‘é‡çŸ©é˜µï¼‰çš„ç»Ÿä¸€
# 2.1 è¯æ±‡è¡¨ä¸­æ‰€æœ‰è¯æ±‡ä¸€å®šæ˜¯ä¸é‡å¤çš„ ç”¨ set
vocab = set() # add() æ·»åŠ å•ä¸ªå…ƒç´   update() ä¸€æ¬¡æ·»åŠ å¤šä¸ªå…ƒç´ 
for comment,vote in comment_data: 
    vocab.update(comment)

vocab = ['PAD','UNK'] + list(vocab) # list + ç­‰ä»·äº  extend()
print(len(vocab))

# å¯ä»¥æŠŠè¯æ±‡è¡¨ç”Ÿæˆå°è£…èµ·æ¥ 

# class Vocabulary():
#     def __init__(self,word):
#         self.vocab = vocab
    
#     # é‡‡ç”¨è£…é¥°å™¨æ¨¡å¼ @classmethod ä½œç”¨åœ¨äº å¯ä»¥åœ¨ä¸å®ä¾‹åŒ–å¯¹è±¡çš„å‰æä¸‹ç›´æ¥è°ƒç”¨å¯¹è±¡é‡Œçš„å‡½æ•°
#     def build_from_doc(cls,doc):
#         vocab = set()
#         for comment,vote in doc:
#             vocab.update(comment)
        
#         # PAD: padding å¡«å…… å¤„ç†æ–‡æ¡£å¤šä¸ªå¥å­è¯æ±‡ä¸ªæ•°ä¸ä¸€è‡´å¯¼è‡´çš„æ— æ³•æ‰¹æ¬¡è®­ç»ƒçš„ é—®é¢˜  UNK: unknow å¤„ç†OOVé—®é¢˜
#         #å¡«å……å€¼ä¸åº”è¯¥å‚ä¸åˆ°æ¨¡å‹çš„è®­ç»ƒä¸­å» ä½†æ˜¯ ä¸ºäº†æ‰¹æ¬¡è®­ç»ƒ åªæœ‰å¡«å……ä¸ºçœ‹é½ æ‰€ä»¥å–0ã€
#         # ä¸€èˆ¬paddingåˆ° æ•´ä¸ªæ–‡æ¡£ä¸­ æœ€é•¿å¥å­è¯æ±‡ä¸ªæ•°çš„é•¿åº¦ 
#         vocab = ['PAD','UNK'] + list(vocab) # list() ä¸ºäº†æœ‰åºåŒ– setçš„åº•å±‚æ˜¯hashæ— åº
        
#         return cls(vocab)


# vocab = Vocabulary.build_from_doc(comment_data)

# 2.2 å°†è¯æ±‡è¡¨è½¬åŒ–ä¸ºç´¢å¼•
wd2idx = {word : index for index,word in enumerate(vocab)}
# print(list(wd2idx.items())[:5])

# 2.3 å°†ç´¢å¼•è½¬æ¢ä¸ºå‘é‡
# é¦–å…ˆ è¦æœ‰ä¸€ä¸ªå¤§å‹çš„æ‰€æœ‰å‘é‡çš„é›†åˆ Embedding(è¯åµŒå…¥)
# å¦‚ä½•ç†è§£åµŒå…¥ï¼Ÿ ç”¨å¤§ç»´åº¦ç©ºé—´ä¸Šçš„ä¸€ä¸ªç‚¹ ä»£è¡¨ä¸€ä¸ªæ•°æ®è¯æ±‡  ä¸” æœ‰è®¡ç®—çš„åŠŸèƒ½
emb = nn.Embedding(len(vocab),100)  # Embedding(vocab_length,embedding_dim)

# 3 è½¬æ–‡æœ¬ä¸ºè¯çš„ç´¢å¼•åºåˆ—

# å‡ºç°OOVé—®é¢˜ æ— æ³•åœ¨è¯è¡¨ä¸­æ‰¾åˆ°è¦è®­ç»ƒçš„æ–‡æœ¬ä¸­çš„è¯
# æ¯”å¦‚
# text_idx = [wd2idx[word] for word in 'æµ‹è¯• æ–‡æœ¬ è½¬æ¢ ä¸º ç´¢å¼• åºåˆ— ğŸ˜€ ğŸ¥§'.split()]
# å¦‚ä½•å¤„ç†OOV


texts_idx = []
for cmt in comment_data:
    text_idx = [wd2idx.get(word,wd2idx['UNK']) for word in cmt[0]]
    texts_idx.append(torch.tensor(text_idx)) # ç´¢å¼•åºåˆ— è½¬ è¯å‘é‡ ç´¢å¼•åºåˆ—å¿…é¡»æ˜¯tensorç±»å‹

#print(texts_idx[:2])

# é€šè¿‡dataset æ„å»º dataloader
# dataloader = DataLoader(comment_data,batch_size = 32,shuffle = True)
# å¦‚ä½•ç”¨dataloaderè§£å†³æ•°æ®é•¿åº¦å¡«å……çš„é—®é¢˜ collate_fnæ–¹æ³• ä¸€ç§å›è°ƒçš„æ–¹æ³• åŸç†æ˜¯åœ¨æ•°æ®æ‰“åŒ…ä¹‹å ä¼ ç»™æ¨¡å‹ä¹‹å‰ collate_fnå†æ¬¡åŠ å·¥æ•´ç†
# 1 è‡ªå®šä¹‰æ•°æ®è½¬æ¢æ–¹æ³•(call_back function)å›è°ƒå‡½æ•° ä¸ç”±è‡ªå·±è°ƒç”¨ ç”±ç³»ç»Ÿè°ƒç”¨
# è¯¥æ–¹æ³•ä¼šåœ¨æ¯ä¸ªbatchæ•°æ®åŠ è½½æ—¶è°ƒç”¨
def convert_data(batch_data):
    # print("custom method invoked")
    # print(batch_data)
    #åˆ†åˆ«æå–è¯„è®ºå’Œæ ‡ç­¾
    comments,votes = [],[]
    for comment,vote in batch_data:
        comments.append(torch.tensor([wd2idx.get(word,wd2idx['UNK']) for word in comment]))
        votes.append(vote)   # å†™æˆ votes.append(torch.tensor(vote)) æœ€åçš„labelsæ˜¯[tensor(0), tensor(0), tensor(0), tensor(0)]å½¢å¼çš„åˆ—è¡¨ è€Œä¸”è®­ç»ƒéœ€è¦å¼ é‡

    # å¡«å……å¼ é‡é•¿åº¦ padding_value é»˜è®¤æ˜¯0
    cmmts = pad_sequence(comments,batch_first = True,padding_value = wd2idx['PAD']) 
    labels = torch.tensor(votes)

    return cmmts,labels

dataloader = DataLoader(comment_data,batch_size = 4,shuffle = True,collate_fn = convert_data)



# # è¯•éªŒä¸€æ¬¡ dataloader collate_fn
# for cmt,label in dataloader:
#     print(cmt,label)
#     break

# # # æ–‡æœ¬ç´¢å¼•åºåˆ—è½¬å‘é‡çŸ©é˜µ
# # sentence_emb = emb(texts_idx[0])
# # print(sentence_emb.shape)


# æ¨¡å‹çš„æ„å»º åŒ…æ‹¬ embeding åœ¨æ¨¡å‹çš„æ­å»ºä¸­ æ­¤æ—¶æˆ‘ä»¬çš„æ¨¡å‹æ­å»ºä¸å†æ˜¯ä¹‹å‰çš„å•çº¯çš„ç½‘ç»œå±‚æ­å»º
class Comments_Classifier(nn.Module):
    def __init__(self,vocab_size,embedding_dim,hidden_size,num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size,embedding_dim,padding_idx = 0) # padding_idx æŒ‡å®špadçš„ç´¢å¼•å€¼ é»˜è®¤none æ­¤å¤„å¡«ä¸Šçš„å¥½å¤„åœ¨äº é¿å…å¡«å……å€¼0ç´¢å¼•å¯¹åº”çš„å‘é‡å‚ä¸åˆ°æ¨¡å‹çš„è®­ç»ƒçš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­
        self.rnn = nn.LSTM(embedding_dim,hidden_size,batch_first = True)
        self.fc = nn.Linear(hidden_size,num_classes)

    def forward(self,input_idx):
        # input_idx: (batch_size,sequence_len)
        # embedding: (batch_size,sequene_len,embedding_dim)
        embedding = self.embedding(input_idx)
        # output: (batch_sizem,seq_len,hidden_size)
        output,(hidden,_) =  self.rnn(embedding)
        output = self.fc(output[:,-1,:])
        return output

# æ¨¡å‹çš„ç”Ÿæˆ
vocab_size = len(vocab)
embedding_dim = 100
hidden_size = 128
num_classes = 2

model = Comments_Classifier(len(vocab),embedding_dim,hidden_size,num_classes)
print(model)

# æ¨¡å‹å‚æ•°
EPOCH = 5
LR = 0.01

# æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
crition = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(),lr = LR)

# å¼€å§‹è®­ç»ƒ
for epoch in range(EPOCH):
    for i,(cmt,label) in enumerate(dataloader):
        #å‰å‘ä¼ æ’­
        output = model(cmt)
        # è®¡ç®—æŸå¤±
        loss = crition(output,label)
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦ä¸‹é™ && å‚æ•°æ›´æ–°
        optimizer.step()
        if (i + 1) % 10 == 0:
            print(f'epoch:{epoch + 1}/{EPOCH},step:{(i + 1)}/{len(dataloader)},loss:{loss.item()}')


# ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(),'comments_classifier.pth')

# ä¿å­˜è¯å…¸
torch.save(wd2idx,'comments_vocab.pth') # è®­ç»ƒå®Œä¸€ä¸ªæ¨¡å‹è¦ä¿å­˜å¯¹åº”çš„è¯å…¸ å› ä¸ºä¸åŒçš„è®­ç»ƒ å‡ºæ¥çš„è¯å…¸æ˜¯ä¸ä¸€æ ·çš„
